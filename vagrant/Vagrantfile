# -*- mode: ruby -*-
# vi: set ft=ruby :

# Define the list of machines in the cluster
slurm_cluster = {
    :node00 => {
        :hostname => "node00",
        :ipaddress => "10.10.10.4",
        :type => "node"
    },
    :node01 => {
        :hostname => "node01",
        :ipaddress => "10.10.10.5",
        :type => "node"
    },
    :headnode => {
        :hostname => "headnode",
        :ipaddress => "10.10.10.3",
        :type => "controller"
    }
}

# Provisioning inline script for all nodes
$nodescript = <<NODESCRIPT
set -x
yum -y upgrade
yum -y install epel-release 
yum -y install pdsh

sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
systemctl restart sshd

NODESCRIPT



# provisioning script for the head node
$headnode_script = <<HEADNODESCRIPT
set -xe

yum -y upgrade
yum -y install epel-release
yum -y install stress-ng pdsh bind-utils expect gcc make sshpass git gtk2-devel hdf5-devel hwloc hwloc-devel hwloc-gui libibmad libibumad libcurl libssh2-devel lua lua-devel man man2html mariadb mariadb-devel mariadb-server munge munge-devel munge-libs net-tools ncurses-devel nfs-utils numactl numactl-devel openssh-server openssl-devel pam-devel perl-ExtUtils-MakeMaker python-pip readline-devel rpm-build rrdtool-devel screen vim wget rpm-devel perl-Switch postfix mailx

if [[ ! -e "/etc/PDSH" ]]; then
    mkdir /etc/PDSH
fi

if [[ ! -f "/etc/PDSH/hosts" ]]; then
printf 'node00\nnode01\n' > /etc/PDSH/hosts
echo 'export WCOLL=/etc/PDSH/hosts' >> /etc/profile
fi

source /etc/profile

sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
systemctl restart sshd

echo "slurm" > /tmp/pass.txt

for node in node00 node01
do 
    sshpass -f /tmp/pass.txt ssh-copy-id -f -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa root@$node
done

echo "Setting up munge key"
dd if=/dev/urandom of=/etc/munge/munge.key iflag=fullblock bs=1024 count=1
chown munge:munge /etc/munge/munge.key
chmod 600 /etc/munge/munge.key

systemctl daemon-reload
systemctl enable munge
systemctl start munge

systemctl enable postfix
systemctl start postfix


if ls /root/rpmbuild/RPMS/x86_64/*.rpm >/dev/null 2>&1; then
  echo "RPM already exist"
else
  cd /root/
  wget -q -O slurm-19.05.0.tar.bz2 https://download.schedmd.com/slurm/slurm-19.05.0.tar.bz2 && \
  rpmbuild -ta slurm-19.05.0.tar.bz2 && \
  rpm -Uvh /root/rpmbuild/RPMS/x86_64/*.rpm   
fi

cp /vagrant/slurm.conf /etc/slurm/slurm.conf
cp /vagrant/slurmdbd.conf /etc/slurm/slurmdbd.conf

systemctl enable slurmdbd.service
systemctl enable slurmctld.service
systemctl enable mariadb

systemctl start mariadb
systemctl status mariadb

mysql -e "DROP USER 'slurm'@'localhost'" >/dev/null 2>&1 || true
mysql -e "CREATE USER 'slurm'@'localhost' identified by 'password';"
mysql -e "GRANT ALL ON slurm_acct_db.* TO 'slurm'@'localhost';"


if [[ ! -e "/var/log/slurm" ]]; then
    mkdir /var/log/slurm
fi

chown slurm:slurm /var/log/slurm/

systemctl start slurmdbd
sleep 5
systemctl status slurmdbd


if [[ ! -e "/var/spool/slurmd" ]]; then
    mkdir /var/spool/slurmd
    chown slurm:slurm /var/spool/slurmd
fi

if [[ ! -e "/var/spool/slurmctld" ]]; then
    mkdir /var/spool/slurmctld
    chown slurm:slurm /var/spool/slurmctld
fi


export WCOLL=/etc/PDSH/hosts
pdsh date
pdcp "/root/rpmbuild/RPMS/x86_64/slurm-slurmd-19.05.0-1.el7.x86_64.rpm" /tmp/
pdcp "/root/rpmbuild/RPMS/x86_64/slurm-19.05.0-1.el7.x86_64.rpm" /tmp/

pdsh 'yum install -y /tmp/slurm-19.05.0-1.el7.x86_64.rpm' | dshbak -c
pdsh 'yum install -y /tmp/slurm-slurmd-19.05.0-1.el7.x86_64.rpm' | dshbak -c

pdsh "systemctl enable slurmd"


echo "Sending munge.key out to the compute nodes"
pdcp /etc/munge/munge.key /etc/munge
pdsh "chown munge:munge /etc/munge/munge.key"
pdsh "chmod 400 /etc/munge/munge.key"


echo "Sending the slurm.conf out to the compute nodes and restarting slurmd"
pdsh "mkdir -p /etc/slurm"
pdcp /etc/slurm/slurm.conf /etc/slurm
pdsh "systemctl restart slurmd"


CLUSTER=`sacctmgr -i list cluster | grep cluster`
if [[ -n $CLUSTER ]]; then
  sacctmgr -i delete cluster cluster
else
  sacctmgr -i create cluster cluster
  sacctmgr -i create account cluster_account
  sacctmgr -i create user steve account=cluster_account
fi

systemctl start slurmctld
systemctl status slurmctld

echo "Run scontrol reconfigure"
scontrol reconfigure


HEADNODESCRIPT

# for node in node00 node01; do ssh-keyscan -H $node >> /root/.ssh/known_hosts; done



Vagrant.configure("2") do |global_config|
    # initial setup of headnode in the cluster
    slurm_cluster.each_pair do |name, options|

        # initial setup of each VM in the cluster
        global_config.vm.define name do |config|
            # VM configurations
            # config.vm.box = "generic/rhel7"
            config.vm.box = "centos/7"
            config.vm.hostname = "#{name}"
            config.vm.network :private_network, ip: options[:ipaddress]

            # VM specifications
            config.vm.provider :virtualbox do |v|
                v.cpus = 2
                v.memory = 1024
            end

            config.vm.provision "shell", privileged: true do |s|
              ssh_prv_key = ""
              ssh_pub_key = ""
              if File.file?("id_rsa")
                  ssh_prv_key = File.read("id_rsa")
                  ssh_pub_key = File.readlines("id_rsa.pub").first.strip
              else
                  puts "No SSH key found. You will need to remedy this before pushing to the repository."
              end
              s.inline = <<-SHELL
                if grep -sq "#{ssh_pub_key}" /root/.ssh/authorized_keys; then
                  echo "SSH keys already provisioned."
                  exit 0;
                fi
                echo "SSH key provisioning."
                mkdir -p /root/.ssh/
                touch /root/.ssh/authorized_keys
                echo #{ssh_pub_key} >> /root/.ssh/authorized_keys
                echo #{ssh_pub_key} > /root/.ssh/id_rsa.pub
                chmod 644 /root/.ssh/id_rsa.pub
                echo "#{ssh_prv_key}" > /root/.ssh/id_rsa
                chmod 600 /root/.ssh/id_rsa

                useradd -m slurm

                echo "10.10.10.3    headnode" >> /etc/hosts
                echo "10.10.10.4    node00" >> /etc/hosts
                echo "10.10.10.5    node01" >> /etc/hosts
                
                echo "slurm" | passwd --stdin root

              SHELL
            end

            if options[:type] == "controller"
                config.vm.provision "shell", privileged: true, :inline => $headnode_script
            else
                config.vm.provision "shell", privileged: true, :inline => $nodescript
            end

        end
    end
end
